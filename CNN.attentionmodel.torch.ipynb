{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    r\"\"\"\n",
    "    3x3 convolution with padding\n",
    "    - in_planes: in_channels\n",
    "    - out_channels: out_channels\n",
    "    - bias=False: BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정.\n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        r\"\"\"\n",
    "         - inplanes: input channel size\n",
    "         - planes: output channel size\n",
    "         - groups, base_width: ResNext나 Wide ResNet의 경우 사용\n",
    "        \"\"\"\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "            \n",
    "        # Basic Block의 구조\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)  # conv1에서 downsample\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # short connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        # identity mapping시 identity mapping후 ReLU를 적용합니다.\n",
    "        # 그 이유는, ReLU를 통과하면 양의 값만 남기 때문에 Residual의 의미가 제대로 유지되지 않기 때문입니다.\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion = 4 # 블록 내에서 차원을 증가시키는 3번째 conv layer에서의 확장계수\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        # ResNext나 WideResNet의 경우 사용\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        \n",
    "        # Bottleneck Block의 구조\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation) # conv2에서 downsample\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        # 1x1 convolution layer\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        # 3x3 convolution layer\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        # 1x1 convolution layer\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        # skip connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "        # default values\n",
    "        self.inplanes = 64 # input feature map\n",
    "        self.dilation = 1\n",
    "        # stride를 dilation으로 대체할지 선택\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        \n",
    "        r\"\"\"\n",
    "        - 처음 입력에 적용되는 self.conv1과 self.bn1, self.relu는 모든 ResNet에서 동일 \n",
    "        - 3: 입력으로 RGB 이미지를 사용하기 때문에 convolution layer에 들어오는 input의 channel 수는 3\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        r\"\"\"\n",
    "        - 아래부터 block 형태와 갯수가 ResNet층마다 변화\n",
    "        - self.layer1 ~ 4: 필터의 개수는 각 block들을 거치면서 증가(64->128->256->512)\n",
    "        - self.avgpool: 모든 block을 거친 후에는 Adaptive AvgPool2d를 적용하여 (n, 512, 1, 1)의 텐서로\n",
    "        - self.fc: 이후 fc layer를 연결\n",
    "        \"\"\"\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, # 여기서부터 downsampling적용\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        r\"\"\"\n",
    "        convolution layer 생성 함수\n",
    "        - block: block종류 지정\n",
    "        - planes: feature map size (input shape)\n",
    "        - blocks: layers[0]와 같이, 해당 블록이 몇개 생성돼야하는지, 블록의 갯수 (layer 반복해서 쌓는 개수)\n",
    "        - stride와 dilate은 고정\n",
    "        \"\"\"\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        \n",
    "        # the number of filters is doubled: self.inplanes와 planes 사이즈를 맞춰주기 위한 projection shortcut\n",
    "        # the feature map size is halved: stride=2로 downsampling\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # 블록 내 시작 layer, downsampling 필요\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion # inplanes 업데이트\n",
    "        # 동일 블록 반복\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suhancho/miniforge3/envs/pytorch/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/suhancho/miniforge3/envs/pytorch/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: '@rpath/libpng16.16.dylib'\n",
      "  Referenced from: '/Users/suhancho/miniforge3/envs/pytorch/lib/python3.10/site-packages/torchvision/image.so'\n",
      "  Reason: tried: '/Users/ec2-user/actions-runner/_work/vision/vision/conda-env-2408739877/lib/libpng16.16.dylib' (no such file), '/Users/ec2-user/actions-runner/_work/vision/vision/conda-env-2408739877/lib/libpng16.16.dylib' (no such file), '/Users/ec2-user/actions-runner/_work/vision/vision/conda-env-2408739877/lib/libpng16.16.dylib' (no such file), '/Users/ec2-user/actions-runner/_work/vision/vision/conda-env-2408739877/lib/libpng16.16.dylib' (no such file), '/Users/suhancho/miniforge3/envs/pytorch/lib/libpng16.16.dylib' (no such file), '/Users/suhancho/miniforge3/envs/pytorch/lib/libpng16.16.dylib' (no such file), '/Users/suhancho/miniforge3/envs/pytorch/lib/python3.10/lib-dynload/../../libpng16.16.dylib' (no such file), '/Users/suhancho/miniforge3/envs/pytorch/lib/libpng16.16.dylib' (no such file), '/Users/suhancho/miniforge3/envs/pytorch/bin/../lib/libpng16.16.dylib' (no such file), '/usr/local/lib/libpng16.16.dylib' (no such file), '/usr/lib/libpng16.16.dylib' (no such file)\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConvolutionalResnet18(models.ResNet):\n",
    "    '''\n",
    "    From https://learnopencv.com/fully-convolutional-image-classification-on-arbitrary-sized-image/\n",
    "    Applying flexible input size for ResNet.\n",
    "    '''\n",
    "    def __init__(self, num_classes=30, pretrained=False, **kwargs):\n",
    " \n",
    "        # Start with standard resnet18 defined here \n",
    "        super().__init__(block = models.resnet.BasicBlock, layers = [2, 2, 2, 2], num_classes = num_classes, **kwargs)\n",
    "        if pretrained:\n",
    "            state_dict = load_state_dict_from_url( models.resnet.model_urls[\"resnet18\"], progress=True)\n",
    "            self.load_state_dict(state_dict)\n",
    " \n",
    "        # Replace AdaptiveAvgPool2d with standard AvgPool2d \n",
    "        self.avgpool = nn.AvgPool2d((7, 7))\n",
    " \n",
    "        # Convert the original fc layer to a convolutional layer.  \n",
    "        self.last_conv = torch.nn.Conv2d( in_channels = self.fc.in_features, out_channels = num_classes, kernel_size = 1)\n",
    "        self.last_conv.weight.data.copy_( self.fc.weight.data.view ( *self.fc.weight.data.shape, 1, 1))\n",
    "        self.last_conv.bias.data.copy_ (self.fc.bias.data)\n",
    " \n",
    "    # Reimplementing forward pass. \n",
    "    def _forward_impl(self, x):\n",
    "        # Standard forward for resnet18\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    " \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    " \n",
    "        # Notice, there is no forward pass \n",
    "        # through the original fully connected layer. \n",
    "        # Instead, we forward pass through the last conv layer\n",
    "        x = self.last_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 Arguments\n",
    "## pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "## progress (bool): If True, displays a progress bar of the download to stderr\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    r\"\"\"\n",
    "    - pretrained: pretrained된 모델 가중치를 불러오기 (saved by caffe)\n",
    "    - arch: ResNet모델 이름\n",
    "    - block: 어떤 block 형태 사용할지 (\"Basic or Bottleneck\")\n",
    "    - layers: 해당 block이 몇번 사용되는지를 list형태로 넘겨주는 부분\n",
    "    \"\"\"\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "# 기본 ResNet 34층\n",
    "def resnet34(pretrained=False, progress=True, **kwargs):\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress, **kwargs)\n",
    "\n",
    "# 기본 ResNet 50층\n",
    "def resnet50(pretrained=False, progress=True, **kwargs):\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)\n",
    "\n",
    "# Wide ResNet\n",
    "def wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)\n",
    "\n",
    "# ResNext\n",
    "def resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n",
    "    kwargs['groups'] = 32 # input channel을 32개의 그룹으로 분할 (cardinality)\n",
    "    kwargs['width_per_group'] = 4 # 각 그룹당 4(=128/32)개의 채널으로 구성.\n",
    "    # 각 그룹당 channel 4의 output feautre map 생성, concatenate해서 128개로 다시 생성.\n",
    "    \n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSSMDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,pssm_in,label_in):\n",
    "        self.pssm_lst = sorted(os.listdir(pssm_in))\n",
    "        self.pssm_files = [pssm_in+f for f in self.pssm_lst]\n",
    "        self.label_files = [label_in+f.split('.')[0]+'.label.txt' for f in self.pssm_lst]\n",
    "    def __len__(self):\n",
    "        return len(self.label_files)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.pssm_tensor = torch.tensor(pd.read_table(self.pssm_files[idx],index_col=0).astype(float).values)\n",
    "        self.label_tensor = torch.tensor(list(map(int,open(self.label_files[idx],'r').readlines()[0].split(','))))\n",
    "        return self.pssm_tensor, self.label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    dim_data = [d.size()[0] for d in data]\n",
    "    max_seqlen = max(dim_data)\n",
    "    data_transformed = [F.pad(d.T,(0,int(max_seqlen - d.size()[0]))) if d.size()[0]!=max_seqlen else d.T for d in data]\n",
    "    target = [item[1] for item in batch]\n",
    "    return [data_transformed, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pssminpath = '/Users/suhancho/data/Uniprot_metalbinding_challenge/processed_pssm/'\n",
    "labelinpath = '/Users/suhancho/data/Uniprot_metalbinding_challenge/processed_label/'\n",
    "train_ds = PSSMDataset(pssminpath,labelinpath)\n",
    "train_dl = DataLoader(train_ds,batch_size = 16,shuffle = True,collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_files = [pssminpath+f for f in os.listdir(pssminpath)]\n",
    "# a = torch.tensor(pd.read_table(test_files[1],index_col=0).astype(float).values)\n",
    "# F.pad(a,(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50()\n",
    "model = FullyConvolutionalResnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0000, -3.0000, -4.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-3.0000,  1.0000,  8.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-4.0000, -3.0000, -4.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 1.0000,  1.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.0200,  2.0000,  5.4000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 1.1200,  1.6800,  1.2000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (list, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [190], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# X_batch, y_batch = X_batch.to(device), y_batch.to(device)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_batch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# acc = binary_acc(y_pred, y_batch.unsqueeze(1))\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "Cell \u001b[0;32mIn [106], line 21\u001b[0m, in \u001b[0;36mFullyConvolutionalResnet18._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Standard forward for resnet18\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 459\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/conv.py:455\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    452\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    453\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    454\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 455\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    456\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() received an invalid combination of arguments - got (list, Parameter, NoneType, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, tuple of ints padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n * (Tensor input, Tensor weight, Tensor bias, tuple of ints stride, str padding, tuple of ints dilation, int groups)\n      didn't match because some of the arguments have invalid types: (!list!, !Parameter!, !NoneType!, !tuple!, !tuple!, !tuple!, int)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "EPOCHS = 100\n",
    "model.train()\n",
    "for e in range(1, EPOCHS+1):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_dl:\n",
    "        print(X_batch[5])\n",
    "        # X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch)\n",
    "       \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        # acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    if e%5==0:\n",
    "        print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_dl):.5f} | Acc: {epoch_acc/len(train_dl):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.0000, -1.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.0000,  1.0000,  4.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-3.0000,  4.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  2.2400,  3.5900,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  2.4100,  1.1200,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-1.0000,  2.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000, -2.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  0.8900,  2.2900,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  4.8400,  2.0100,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-1.0000, -1.0000, -1.0000,  ...,  0.0000,  1.0000, -2.0000],\n",
       "         [-1.0000, -2.0000, -1.0000,  ..., -1.0000, -1.0000, -3.0000],\n",
       "         [-2.0000, -3.0000, -2.0000,  ...,  0.0000,  0.0000, -3.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  1.0000,  1.0000,  ...,  0.0000,  0.0000,  1.0000],\n",
       "         [ 1.0200,  1.0200,  1.0200,  ...,  2.2900,  2.2900,  0.9800],\n",
       "         [ 1.1200,  1.1200,  1.1200,  ...,  2.0100,  2.0100,  1.2000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-1.0000, -1.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.0000, -1.0000, -3.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-3.0000, -1.0000, -3.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  1.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  0.9300,  0.9300,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  4.4300,  1.1200,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-1.0000, -1.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000, -2.0000, -2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.0000, -3.0000, -2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  0.9800,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  1.2000,  1.1300,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-2.0000, -3.0000, -4.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-3.0000,  1.0000,  8.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-4.0000, -3.0000, -4.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  1.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  2.0000,  5.4000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  1.6800,  1.2000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-2.0000,  1.0000, -3.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-3.0000, -1.0000, -3.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-3.0000,  3.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  2.3100,  0.9200,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  2.0400,  1.0800,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-1.0000, -2.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.0000, -1.0000,  2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.0000,  6.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  2.2400,  2.2700,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  2.4100,  2.4100,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-2.0000, -2.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-3.0000,  4.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-4.0000, -1.0000,  2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  1.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  3.5900,  3.5900,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  1.1200,  1.1200,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-1.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0000, -3.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.0000, -3.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  0.9600,  2.3100,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  1.1200,  2.0400,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-1.0000,  0.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  1.0000,  2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  3.5900,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  1.1200,  1.1300,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-4.0000, -1.0000, -2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-5.0000, -6.0000,  2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-6.0000, -6.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  0.9300,  3.5900,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  1.1100,  1.1200,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-3.0000, -2.0000, -2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-3.0000, -4.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-4.0000, -5.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  0.9300,  2.3100,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  1.1100,  2.0400,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-3.0000,  0.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-3.0000, -3.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-4.0000, -3.0000, -2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  0.0000,  0.9300,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  1.1300,  1.1100,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-2.0000,  1.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.0000, -1.0000,  2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-3.0000, -1.0000,  3.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  2.2900,  2.2400,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  2.0100,  2.4100,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-1.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.0000, -1.0000, -1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-3.0000,  0.0000,  2.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 1.0000,  0.0000,  1.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.0200,  2.3100,  0.9300,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 1.1200,  2.0400,  4.4300,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9d86feb0499a0b58932a2305ba04f997e477cb03dd1b79441914e4f34b762fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

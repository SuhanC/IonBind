{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from csv import DictReader\n","from random import Random\n","from collections import Counter\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","from tqdm import tqdm\n","from keras.utils.np_utils import to_categorical\n","import pickle as pkl\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"f2cd49442708186b9a201f1df0de45bc461c0acc","trusted":true},"outputs":[],"source":["RANDOM = Random(42)"]},{"cell_type":"markdown","metadata":{"_uuid":"6375e90151f1fa6825c0fffba46fd52431b9196b"},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-input":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["def load_samples(path):\n","    with open(path) as f:\n","        samples = list(DictReader(f))\n","        for sample in samples:\n","            sample['target'] = int(sample.get('target', -1))\n","        return samples"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"c3c4db7c62ab4796c171f7174f2e6345b7d0ce3f","trusted":true},"outputs":[],"source":["def train_val_split(samples, split=0.2):\n","    RANDOM.shuffle(samples)\n","    n_val = int(len(samples) * split)\n","    return samples[:-n_val], samples[-n_val:]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","dat_prot = pd.read_table('/Users/suhancho/data/Uniprot_metalbinding_challenge/sequence_df.tsv')\n","chebi = pd.read_table('/Users/suhancho/data/Uniprot_metalbinding_challenge/POS_TRAIN_FULL.tsv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dat_prot_bindsite = pd.merge(dat_prot,chebi,left_on = 'Protein name',right_on='Accession',how = 'outer')\n","neg_seqs = dat_prot_bindsite[dat_prot_bindsite.Type=='neg_sequence']\n","neg_seqs['Position'] = [int(divmod(len(p),2)[0]) for p in neg_seqs['Protein sequence'].tolist()]\n","neg_seqs['ChEBI-ID'] = 'NB'\n","dat_prot_bindsite = pd.concat([dat_prot_bindsite[dat_prot_bindsite['Type']=='pos_sequence'],neg_seqs]).reset_index(drop = True)\n","dat_prot_bindsite['ChEBI-ID'] = pd.Categorical(dat_prot_bindsite['ChEBI-ID'])\n","dat_prot_bindsite['target'] = dat_prot_bindsite['ChEBI-ID'].cat.codes\n","dat_prot_bindsite['Position'] = dat_prot_bindsite['Position'].astype(int)\n","dat_prot_bindsite_sampled = dat_prot_bindsite.sample(frac=1,random_state=9510)\n","# dat_prot_bindsite_sampled.to_csv('/Users/suhancho/data/Uniprot_metalbinding_challenge/data_before_windowing.tsv',sep='\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_windowdf(protfile,winsize=4):\n","    window=[]\n","    with open(protfile,'r') as p:\n","        for line in tqdm(p):\n","            if not line.count('Protein sequence'):\n","                protseq = line.split('\\t')[2].strip()\n","                bindsite = int(line.split('\\t')[-2].strip())\n","\n","                if ((bindsite-winsize)<0 )& ((bindsite+winsize) > len(protseq)) :\n","                    front_pad_seq = 'X'*(winsize-bindsite)\n","                    bindseq = protseq[bindsite-winsize : bindsite+winsize]\n","                    rear_pad_seq = 'X'*((winsize+bindsite)-len(protseq))\n","                    windowed = front_pad_seq+bindseq+rear_pad_seq\n","\n","                elif ((bindsite-winsize)<0) & ((bindsite+winsize) < len(protseq)):\n","                    front_pad_seq = 'X'*(winsize-bindsite)\n","                    bindseq = protseq[bindsite-winsize : bindsite+winsize]\n","                    windowed = front_pad_seq+bindseq\n","\n","                elif ((bindsite-winsize)>0) & ((bindsite+winsize) > len(protseq)):\n","                    rear_pad_seq = 'X'*((winsize+bindsite)-len(protseq))\n","                    bindseq = protseq[bindsite-winsize : bindsite+winsize]\n","                    windowed = bindseq+rear_pad_seq\n","\n","                else:\n","                    windowed = protseq[bindsite-winsize : bindsite+winsize]\n","                    \n","                window.append(windowed)\n","    return(window)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test = get_windowdf('/Users/suhancho/data/Uniprot_metalbinding_challenge/data_before_windowing.tsv',winsize = 4)\n","dat_prot_bindsite_sampled['window_4'] = test\n","dat_prot_bindsite_sampled = dat_prot_bindsite_sampled.reset_index()\n","dat_prot_bindsite_sampled.rename(columns = {'index':'qid','window_4':'question_text'},inplace = True)\n","dat_prot_bindsite_sampled['qid'] = 'HASH_'+dat_prot_bindsite_sampled['qid'].astype(str)\n","one_hot_labels = to_categorical(dat_prot_bindsite_sampled['target'].tolist())\n","dat_prot_bindsite_sampled['target'] = list(one_hot_labels)\n","samples=dat_prot_bindsite_sampled[['qid','question_text','target']].to_dict('records')\n","train_samples, val_samples = train_val_split(samples)"]},{"cell_type":"markdown","metadata":{"_uuid":"0a91472b0fba9ee2cfa7082d9fbc54da16cee05f"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"759aba1d10c5942009ba1540b71604be7bdcbdcb","trusted":true},"outputs":[],"source":["def build_vocabulary(samples, vocab_min_freq=100):\n","    counts = Counter(ch for sample in samples for ch in sample['question_text'])\n","    chars = sorted(ch for ch, count in counts.items() if count >= vocab_min_freq)\n","    return {char: i for i, char in enumerate(chars)}"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"cea3ed30248f4fa394441ca98296c1bb1de485ad","trusted":true},"outputs":[],"source":["vocabulary = build_vocabulary(train_samples)\n","# print(len(vocabulary))\n","# vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"0399fafa54b3210aa5de3cc776943c2135776605","trusted":true},"outputs":[],"source":["def transform(sample, vocabulary):\n","    sample['encoded_text'] = [vocabulary[ch] for ch in sample['question_text'] if ch in vocabulary]\n","    return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3112df9441d7d951e0e759fb1fcca1d09d93048a","trusted":true},"outputs":[],"source":["train_samples = [transform(sample, vocabulary) for sample in train_samples]\n","val_samples = [transform(sample, vocabulary) for sample in val_samples]"]},{"cell_type":"markdown","metadata":{"_uuid":"0ccbddf1488f16c8842ff260969dd540ab7397a8"},"source":["## Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import keras\n","from keras import backend as K\n","from keras import initializers, regularizers, constraints\n","# from keras.engine import Layer\n","# from tensorflow.keras.layers import Layer\n","def dot_product(x, kernel):\n","    \"\"\"\n","    Wrapper for dot product operation, in order to be compatible with both\n","    Theano and Tensorflow\n","    Args:\n","        x (): input\n","        kernel (): weights\n","    Returns:\n","    \"\"\"\n","    if K.backend() == 'tensorflow':\n","        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n","    else:\n","        return K.dot(x, kernel)\n","# from keras.layers import InputLayer, Input\n","\n","class AttentionWithContext(keras.layers.Layer):\n","    \"\"\"\n","    Attention operation, with a context/query vector, for temporal data.\n","    Supports Masking.\n","    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n","    \"Hierarchical Attention Networks for Document Classification\"\n","    by using a context vector to assist the attention\n","    # Input shape\n","        3D tensor with shape: `(samples, steps, features)`.\n","    # Output shape\n","        2D tensor with shape: `(samples, features)`.\n","\n","    How to use:\n","    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n","    The dimensions are inferred based on the output shape of the RNN.\n","\n","    Note: The layer has been tested with Keras 2.0.6\n","\n","    Example:\n","        model.add(LSTM(64, return_sequences=True))\n","        model.add(AttentionWithContext())\n","        # next add a Dense layer (for classification/regression) or whatever...\n","    \"\"\"\n","\n","    def __init__(self,\n","                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, u_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","\n","        self.supports_masking = True\n","        self.init = initializers.get('glorot_uniform')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.u_regularizer = regularizers.get(u_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.u_constraint = constraints.get(u_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        super(AttentionWithContext, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight(shape = (input_shape[-1], input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        if self.bias:\n","            self.b = self.add_weight(shape = (input_shape[-1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","\n","        self.u = self.add_weight(shape = (input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_u'.format(self.name),\n","                                 regularizer=self.u_regularizer,\n","                                 constraint=self.u_constraint)\n","\n","        super(AttentionWithContext, self).build(input_shape)\n","\n","    def compute_mask(self, input, input_mask=None):\n","        # do not pass the mask to the next layers\n","        return None\n","\n","    def call(self, x, mask=None):\n","        uit = dot_product(x, self.W)\n","\n","        if self.bias:\n","            uit += self.b\n","\n","        uit = K.tanh(uit)\n","        ait = dot_product(uit, self.u)\n","\n","        a = K.exp(ait)\n","\n","        # apply mask after the exp. will be re-normalized next\n","        if mask is not None:\n","            # Cast the mask to floatX to avoid float64 upcasting in theano\n","            a *= K.cast(mask, K.floatx())\n","\n","        # in some cases especially in the early stages of training the sum may be almost zero\n","        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n","        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0], input_shape[-1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b00234b1283b37886bb9e1f5307ed01a3a79ecb2","trusted":true},"outputs":[],"source":["from keras import models, Model\n","from keras.layers import Input, Embedding, Conv1D, Add, Dense, SpatialDropout1D\n","\n","def _conv_block(x, filters, kernel_size):\n","    conv = Conv1D(filters, kernel_size, activation='relu', padding='same')(x)\n","    conv = Conv1D(filters, kernel_size, activation='relu', padding='same')(conv)\n","    return conv\n","\n","\n","def _resblock(x, filters, kernel_size):\n","    conv = _conv_block(x, filters, kernel_size)\n","    projection = Conv1D(filters, 1, padding='same')(x)\n","    return Add()([conv, projection])\n","\n","\n","def predict_bindsite(vocab_size,\n","                    char_embedding_size,\n","                    base_filters,\n","                    doc_embedding_size,\n","                    dropout):\n","    text = Input(shape=(None,))\n","    embedding = Embedding(vocab_size, char_embedding_size)(text)\n","\n","    conv_1 = _resblock(embedding, base_filters, 3)\n","    conv_1 = SpatialDropout1D(dropout)(conv_1)\n","    conv_2 = _resblock(conv_1, base_filters * 2, 3)\n","    conv_2 = SpatialDropout1D(dropout)(conv_2)\n","    conv_3 = _resblock(conv_2, base_filters * 4, 3)\n","    conv_3 = SpatialDropout1D(dropout)(conv_3)\n","    conv_4 = _resblock(conv_3, base_filters * 8, 3)\n","    conv_4 = SpatialDropout1D(dropout)(conv_4)\n","\n","    attention = AttentionWithContext()(conv_4)\n","\n","    fc_1 = Dense(doc_embedding_size, activation='relu')(attention)\n","    fc_2 = Dense(doc_embedding_size, activation='relu')(fc_1)\n","    # prediction = Dense(1, activation='sigmoid')(fc_2)\n","    # prediction = Dense(29, activation='sigmoid')(fc_2)\n","    prediction = Dense(30, activation='sigmoid')(fc_2) # Originally sigmoid\n","\n","    model = Model(text, prediction)\n","    # model.compile('adam', 'binary_crossentropy', metrics=['acc'])\n","    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"_uuid":"9c48deb6eb418c4e0fefb4321d8ec66d83633d72"},"source":["## Training Support"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c80edd9ecc0469f7ba9b0b8e26f5d2c894f2e2ed","trusted":true},"outputs":[],"source":["from math import ceil\n","\n","class BatchProvider:\n","    def __init__(self, samples, batch_size, shuffle=False, run_forever=False):\n","        self._samples = samples\n","        self._batch_size = batch_size\n","        self._shuffle = shuffle\n","        self._run_forever = run_forever\n","\n","    def generate_batches(self):\n","        batch = []\n","        indices = list(range(len(self._samples)))\n","        while True:\n","            if self._shuffle:\n","                RANDOM.shuffle(indices)\n","            for i in indices:\n","                batch.append(self.get_item(i))\n","                if len(batch) == self._batch_size:\n","                    yield self.transform_batch(batch)\n","                    batch = []\n","            if not self._run_forever:\n","                break\n","        if batch:\n","            yield self.transform_batch(batch)\n","\n","    def __len__(self):\n","        return int(ceil(len(self._samples) / self._batch_size))\n","\n","    def get_item(self, idx):\n","        sample = self._samples[idx]\n","        return sample['encoded_text'], sample['target']\n","\n","    def transform_batch(self, items):\n","        texts, targets = zip(*items)\n","        max_length = max(len(text) for text in texts)\n","        text_batch = np.zeros((len(texts), max_length))\n","        for i, text in enumerate(texts):\n","            text_batch[i, :len(text)] = text\n","        target_batch = np.array(targets)\n","        return text_batch, target_batch"]},{"cell_type":"markdown","metadata":{"_uuid":"9c48deb6eb418c4e0fefb4321d8ec66d83633d72"},"source":["## Inference Support"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c80edd9ecc0469f7ba9b0b8e26f5d2c894f2e2ed","trusted":true},"outputs":[],"source":["from math import ceil\n","\n","class TestBatchProvider:\n","    def __init__(self, samples, batch_size, shuffle=False, run_forever=False):\n","        self._samples = samples\n","        self._batch_size = batch_size\n","        self._shuffle = shuffle\n","        self._run_forever = run_forever\n","\n","    def generate_batches(self):\n","        batch = []\n","        indices = list(range(len(self._samples)))\n","        while True:\n","            if self._shuffle:\n","                RANDOM.shuffle(indices)\n","            for i in indices:\n","                batch.append(self.get_item(i))\n","                if len(batch) == self._batch_size:\n","                    yield self.transform_batch(batch)\n","                    batch = []\n","            if not self._run_forever:\n","                break\n","        if batch:\n","            yield self.transform_batch(batch)\n","\n","    def __len__(self):\n","        return int(ceil(len(self._samples) / self._batch_size))\n","\n","    def get_item(self, idx):\n","        sample = self._samples[idx]\n","        return sample['encoded_text']\n","\n","    def transform_batch(self, items):\n","        texts= items\n","        max_length = max(len(text) for text in texts)\n","        text_batch = np.zeros((len(texts), max_length))\n","        for i, text in enumerate(texts):\n","            text_batch[i, :len(text)] = text\n","        return text_batch"]},{"cell_type":"markdown","metadata":{"_uuid":"965761540885c8c9f94fbd18296ee2415edf8f3c"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e4dfbf9e3e87cad230b8f72719ce71e1e94303fb","trusted":true},"outputs":[],"source":["model = predict_bindsite(\n","    vocab_size=len(vocabulary),\n","    char_embedding_size=16,\n","    base_filters=32,#32\n","    doc_embedding_size=300,\n","    dropout=0.1\n",")\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1edff22ca083ce90322122f6766efccb3c51e397","trusted":true},"outputs":[],"source":["train_batch_provider = BatchProvider(train_samples, batch_size=2048, shuffle=True, run_forever=True)\n","train_batches = train_batch_provider.generate_batches()\n","val_batch_provider = BatchProvider(val_samples, batch_size=2048, shuffle=False, run_forever=True)\n","val_batches = val_batch_provider.generate_batches()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4441866a3c94e2ca359fd9b87dfac1ea82e46e18","trusted":true},"outputs":[],"source":["from keras.callbacks import ModelCheckpoint, EarlyStopping\n","EPOCHS = 10\n","model_checkpoint_callback = ModelCheckpoint('./weights.hdf5',monitor='val_acc',save_best_only=True)\n","\n","model.fit_generator(\n","        generator=train_batches,\n","        steps_per_epoch=len(train_batch_provider),\n","        epochs=EPOCHS,\n","        validation_data=val_batches,\n","        validation_steps=len(val_batch_provider),\n","        callbacks= model_checkpoint_callback\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7a900ae6d2f4acbb51cf6d575292a871b481b8eb","trusted":true},"outputs":[],"source":["# model.load_weights('./weights.hdf5')\n","# model.save('./Saved_Ionbind_NLP_221210')\n","model = keras.models.load_model(\"Saved_Ionbind_NLP_221210\")"]},{"cell_type":"markdown","metadata":{"_uuid":"ad3b029bee337c9e6b24d333f231019d610888be"},"source":["## Calibration"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f80b3326275604524a6279aec1b0e30e81812c1c","trusted":true},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","val_batch_provider = BatchProvider(val_samples, batch_size=64, shuffle=False, run_forever=False)\n","val_predictions = model.predict_generator(val_batch_provider.generate_batches(), steps=len(val_batch_provider))\n","val_targets = np.array([sample['target'] for sample in val_samples])\n","val_prediction_argmax = [np.argmax(p) for p in val_predictions]\n","val_targets_argmax = [np.argmax(p) for p in val_targets]\n","\n","report = classification_report(val_targets_argmax,val_prediction_argmax,output_dict = True)\n","report = pd.DataFrame(report).T"]},{"cell_type":"markdown","metadata":{},"source":["# Target ion information processing and result processing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_ion_info = dat_prot_bindsite_sampled[['target','ChEBI-ID']]\n","target_ion_info['target'] = target_ion_info['target'].apply(np.argmax).astype(int)\n","target_ion_info.drop_duplicates(inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["report = classification_report(val_targets_argmax,val_prediction_argmax,output_dict = True)\n","report = pd.DataFrame(report)\n","report_class = report.drop(['accuracy','macro avg','weighted avg'],axis=1).T\n","report_class = report_class.reset_index()\n","report_class['index'] = report_class['index'].astype(int)\n","report_class = pd.merge(report_class,target_ion_info,left_on = 'index',right_on = 'target').drop('index',axis=1)\n","report_class = report_class[['target','ChEBI-ID','precision','recall','f1-score','support']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["report_class.head(2)"]},{"cell_type":"markdown","metadata":{"_uuid":"5bcb2fbc865437cb6cc563d21e67a6e95523ccda"},"source":["## Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"59d291977e9cfc894afd4daa7c499d2d36dba914","trusted":true},"outputs":[],"source":["from keras.utils import GeneratorEnqueuer\n","import csv"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# test_dat = pd.read_table('/Users/suhancho/data/Uniprot_metalbinding_challenge/test_sequence_df.tsv')\n","# test_dat['Protein name'] = 'HASH_'+test_dat['Protein name'].astype(str)\n","# test_dat.rename(columns = {'Protein name':'qid'},inplace = True)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["32it [00:03,  8.52it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [84], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m test_batches \u001b[39m=\u001b[39m enqueuer\u001b[39m.\u001b[39mget()\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m test_batches:\n\u001b[0;32m---> 29\u001b[0m     test_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_on_batch(batch)\n\u001b[1;32m     30\u001b[0m     test_prediction_argmax \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39margmax(p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m test_predictions]\n\u001b[1;32m     31\u001b[0m     test_prediction_proba \u001b[39m=\u001b[39m [prob[idx] \u001b[39mfor\u001b[39;00m prob,idx \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(test_predictions,test_prediction_argmax)]\n","File \u001b[0;32m~/miniforge3/envs/keras/lib/python3.8/site-packages/keras/engine/training.py:2474\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2470\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2471\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x\n\u001b[1;32m   2472\u001b[0m     )\n\u001b[1;32m   2473\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_predict_function()\n\u001b[0;32m-> 2474\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[1;32m   2475\u001b[0m \u001b[39mreturn\u001b[39;00m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(outputs)\n","File \u001b[0;32m~/miniforge3/envs/keras/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[0;32m~/miniforge3/envs/keras/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m~/miniforge3/envs/keras/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/miniforge3/envs/keras/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[0;32m~/miniforge3/envs/keras/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[0;32m~/miniforge3/envs/keras/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m~/miniforge3/envs/keras/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["inference_result=[]\n","with open('/Users/suhancho/data/Uniprot_metalbinding_challenge/test_sequence_df.tsv','r') as testfile:\n","    for j,t in tqdm(enumerate(testfile)):\n","        if not t.count('Protein'):\n","            test_input_lst=[]\n","            testseq_tmp = t.split('\\t')[1].strip()\n","            for i in list(range(len(testseq_tmp))):\n","                test_input={} # Generate Dictionary for prediciton\n","                testprot_tmp = 'HASH_'+str(t.split('\\t')[2].strip())+'.'+str(i)\n","                if i-4<0:\n","                    windowed_tmp = 'X'*(4-i)+testseq_tmp[0:i+5]\n","                elif i+5>len(testprot_tmp):\n","                    windowed_tmp = testseq_tmp[i-4:i+5]+'X'*(i+5-len(testseq_tmp)+1)\n","                else : \n","                    windowed_tmp = testseq_tmp[i-4:i+5]\n","\n","                test_input['qid'] = testprot_tmp\n","                test_input['question_text'] = windowed_tmp\n","                test_input_lst.append(test_input)\n","\n","            test_input_transformed = [transform(sample, vocabulary) for sample in test_input_lst]\n","\n","            sample_ids = (sample['qid'] for sample in test_input_lst)\n","            test_batch_provider = TestBatchProvider(test_input_lst, batch_size=len(test_input_lst), shuffle=False, run_forever=False)\n","            enqueuer = GeneratorEnqueuer(test_batch_provider.generate_batches())\n","            enqueuer.start()\n","            test_batches = enqueuer.get()\n","            for batch in test_batches:\n","                test_predictions = model.predict_on_batch(batch)\n","                test_prediction_argmax = [np.argmax(p) for p in test_predictions]\n","                test_prediction_proba = [prob[idx] for prob,idx in zip(test_predictions,test_prediction_argmax)]\n","                inference_result.append([testprot_tmp.split('.')[0].lstrip('HASH_'),test_prediction_argmax,test_prediction_proba])"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0573999</td>\n","      <td>[2, 29, 29, 29, 29, 29, 29, 29, 14, 29, 29, 29...</td>\n","      <td>[0.6886938, 0.9692102, 0.98517925, 0.99607575,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0569043</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.9330529, 0.9998642, 0.9996815, 0.99917895, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0551880</td>\n","      <td>[26, 29, 29, 29, 29, 2, 11, 29, 29, 2, 14, 29,...</td>\n","      <td>[0.8498795, 0.99403894, 0.91398317, 0.9372454,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0257927</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.52239823, 0.9998512, 0.992098, 0.97448486, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0579555</td>\n","      <td>[29, 29, 29, 29, 2, 29, 29, 29, 29, 29, 29, 29...</td>\n","      <td>[0.88516253, 0.992071, 0.98965925, 0.99695176,...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1011827</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 1...</td>\n","      <td>[0.81877303, 0.99829227, 0.99721175, 0.997317,...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0240162</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.8039206, 0.9973279, 0.9967237, 0.95503867, ...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1003604</td>\n","      <td>[2, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29...</td>\n","      <td>[0.97470576, 0.99711055, 0.99036497, 0.9862323...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1006062</td>\n","      <td>[29, 29, 29, 29, 29, 2, 29, 29, 29, 29, 29, 29...</td>\n","      <td>[0.8491071, 0.9980323, 0.98971343, 0.955023, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0864283</td>\n","      <td>[2, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29...</td>\n","      <td>[0.9309748, 0.9939902, 0.98760265, 0.96765125,...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0245704</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.7692641, 0.994286, 0.9894946, 0.972003, 0.9...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0250474</td>\n","      <td>[29, 29, 29, 29, 2, 2, 2, 29, 29, 29, 29, 29, ...</td>\n","      <td>[0.4783211, 0.99284184, 0.7855742, 0.94738394,...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0849930</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.6807793, 0.99727196, 0.9915643, 0.99359155,...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1013312</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 3...</td>\n","      <td>[0.74971104, 0.99960667, 0.9963578, 0.99794894...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0874795</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.8001015, 0.9989724, 0.99702436, 0.9857184, ...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1016574</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.83462393, 0.9998969, 0.9988543, 0.9870264, ...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0255212</td>\n","      <td>[2, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29...</td>\n","      <td>[0.5927265, 0.9957047, 0.97226286, 0.84711456,...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0448318</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.83871657, 0.9987393, 0.99762744, 0.99092466...</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1107915</td>\n","      <td>[24, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.669211, 0.9999064, 0.9921515, 0.89888763, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0958551</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.6912314, 0.9992855, 0.9986966, 0.8867056, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0341815</td>\n","      <td>[24, 29, 2, 29, 29, 29, 29, 29, 29, 29, 26, 29...</td>\n","      <td>[0.54446834, 0.99969935, 0.8649197, 0.9912969,...</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0970884</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 2, 29, 29, 29, 29...</td>\n","      <td>[0.6317295, 0.9967744, 0.9961429, 0.9979968, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0948047</td>\n","      <td>[15, 29, 29, 15, 29, 29, 29, 29, 2, 29, 29, 29...</td>\n","      <td>[0.7757616, 0.99989057, 0.9981318, 0.7851801, ...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>1100446</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.7541525, 0.9968125, 0.99278027, 0.97861576,...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0477688</td>\n","      <td>[15, 29, 29, 15, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.7558574, 0.9991365, 0.99611735, 0.9926751, ...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>1127339</td>\n","      <td>[24, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.6199696, 0.9992391, 0.95725155, 0.99596596,...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0343320</td>\n","      <td>[29, 29, 29, 29, 29, 29, 16, 29, 29, 29, 29, 2...</td>\n","      <td>[0.7052669, 0.99974245, 0.9945858, 0.9970631, ...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0468934</td>\n","      <td>[29, 29, 29, 29, 29, 2, 29, 29, 2, 29, 29, 29,...</td>\n","      <td>[0.646667, 0.9881422, 0.9583848, 0.96064985, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>0361239</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.9570147, 0.9986762, 0.99825144, 0.9957437, ...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>0346546</td>\n","      <td>[29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.7731238, 0.9995639, 0.9909719, 0.9968532, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>0455791</td>\n","      <td>[29, 29, 29, 26, 29, 29, 29, 29, 29, 29, 29, 2...</td>\n","      <td>[0.66343415, 0.9998851, 0.99864036, 0.8938884,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          0                                                  1  \\\n","0   0573999  [2, 29, 29, 29, 29, 29, 29, 29, 14, 29, 29, 29...   \n","1   0569043  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","2   0551880  [26, 29, 29, 29, 29, 2, 11, 29, 29, 2, 14, 29,...   \n","3   0257927  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","4   0579555  [29, 29, 29, 29, 2, 29, 29, 29, 29, 29, 29, 29...   \n","5   1011827  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 1...   \n","6   0240162  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","7   1003604  [2, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29...   \n","8   1006062  [29, 29, 29, 29, 29, 2, 29, 29, 29, 29, 29, 29...   \n","9   0864283  [2, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29...   \n","10  0245704  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","11  0250474  [29, 29, 29, 29, 2, 2, 2, 29, 29, 29, 29, 29, ...   \n","12  0849930  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","13  1013312  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 3...   \n","14  0874795  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","15  1016574  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","16  0255212  [2, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29...   \n","17  0448318  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","18  1107915  [24, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","19  0958551  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","20  0341815  [24, 29, 2, 29, 29, 29, 29, 29, 29, 29, 26, 29...   \n","21  0970884  [29, 29, 29, 29, 29, 29, 29, 2, 29, 29, 29, 29...   \n","22  0948047  [15, 29, 29, 15, 29, 29, 29, 29, 2, 29, 29, 29...   \n","23  1100446  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","24  0477688  [15, 29, 29, 15, 29, 29, 29, 29, 29, 29, 29, 2...   \n","25  1127339  [24, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","26  0343320  [29, 29, 29, 29, 29, 29, 16, 29, 29, 29, 29, 2...   \n","27  0468934  [29, 29, 29, 29, 29, 2, 29, 29, 2, 29, 29, 29,...   \n","28  0361239  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","29  0346546  [29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 2...   \n","30  0455791  [29, 29, 29, 26, 29, 29, 29, 29, 29, 29, 29, 2...   \n","\n","                                                    2  \n","0   [0.6886938, 0.9692102, 0.98517925, 0.99607575,...  \n","1   [0.9330529, 0.9998642, 0.9996815, 0.99917895, ...  \n","2   [0.8498795, 0.99403894, 0.91398317, 0.9372454,...  \n","3   [0.52239823, 0.9998512, 0.992098, 0.97448486, ...  \n","4   [0.88516253, 0.992071, 0.98965925, 0.99695176,...  \n","5   [0.81877303, 0.99829227, 0.99721175, 0.997317,...  \n","6   [0.8039206, 0.9973279, 0.9967237, 0.95503867, ...  \n","7   [0.97470576, 0.99711055, 0.99036497, 0.9862323...  \n","8   [0.8491071, 0.9980323, 0.98971343, 0.955023, 0...  \n","9   [0.9309748, 0.9939902, 0.98760265, 0.96765125,...  \n","10  [0.7692641, 0.994286, 0.9894946, 0.972003, 0.9...  \n","11  [0.4783211, 0.99284184, 0.7855742, 0.94738394,...  \n","12  [0.6807793, 0.99727196, 0.9915643, 0.99359155,...  \n","13  [0.74971104, 0.99960667, 0.9963578, 0.99794894...  \n","14  [0.8001015, 0.9989724, 0.99702436, 0.9857184, ...  \n","15  [0.83462393, 0.9998969, 0.9988543, 0.9870264, ...  \n","16  [0.5927265, 0.9957047, 0.97226286, 0.84711456,...  \n","17  [0.83871657, 0.9987393, 0.99762744, 0.99092466...  \n","18  [0.669211, 0.9999064, 0.9921515, 0.89888763, 0...  \n","19  [0.6912314, 0.9992855, 0.9986966, 0.8867056, 0...  \n","20  [0.54446834, 0.99969935, 0.8649197, 0.9912969,...  \n","21  [0.6317295, 0.9967744, 0.9961429, 0.9979968, 0...  \n","22  [0.7757616, 0.99989057, 0.9981318, 0.7851801, ...  \n","23  [0.7541525, 0.9968125, 0.99278027, 0.97861576,...  \n","24  [0.7558574, 0.9991365, 0.99611735, 0.9926751, ...  \n","25  [0.6199696, 0.9992391, 0.95725155, 0.99596596,...  \n","26  [0.7052669, 0.99974245, 0.9945858, 0.9970631, ...  \n","27  [0.646667, 0.9881422, 0.9583848, 0.96064985, 0...  \n","28  [0.9570147, 0.9986762, 0.99825144, 0.9957437, ...  \n","29  [0.7731238, 0.9995639, 0.9909719, 0.9968532, 0...  \n","30  [0.66343415, 0.9998851, 0.99864036, 0.8938884,...  "]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(inference_result)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 ('keras')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"ea3476042c22507d4a4a83ae58c770fc528ff15454871552e09ef37283ce2491"}}},"nbformat":4,"nbformat_minor":1}
